---
title: "BEAST2"
subtitle: "**B**ayesian **E**volutionary **A**nalysis by **S**ampling **T**rees"
author: "Adrian Zetner"
date: "2019/10/29 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      click: true


---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```
layout: true
class: inverse, center, middle
background-image: url("files/beast/beastcanada-40alpha.png")

---

# Taming the BEAST 2019

## August 12 - 16, 2019

## Squamish, British Columbia

---

## Talks:

Investigating the Dynamics of Diversification using Molecular Phylogenies  
\- Matthew Pennell

Bayesian Outbreak Reconstruction with Unsampled Cases and Tree Uncertainty - Caroline Colijn


Introduction to Infectious Disease Phylodynamics 
\- David Rasmussen  


Evolutionary History of HBV in the Circumpolar Arctic  
--
  \- Carla Oslowy


???

Several invited lecturers presented on studies and techniques with BEAST2

---

## Training:


##### Relaxed Phylogenetics in BEAST 2  
##### Priors with Flu A  
##### Phylogeography in BEAST 2  
##### Population Structure using the Multitype Birth-Death Model  
##### Troubleshooting BEAST 2  

---

##### Many of these tutorials were guided versions of what are available [here](https://taming-the-beast.org/tutorials/)

???


---
## And of course... As always...

---
layout: false
background-image: url(files/magic/coffee.JPG)
background-size: contain

???

Image credit: https://upload.wikimedia.org/wikipedia/commons/thumb/4/45/A_small_cup_of_coffee.JPG/1280px-A_small_cup_of_coffee.JPG

---
class: inverse, center, middle

# BEAST2?

???
Great, so you did some tutorials and watched some talks. Why should we care? What is BEAST2?

---

# BEAST
--

### A cross-platform program for Bayesian analysis of molecular sequences using MCMC.
???

So beast, the noun can be defined as:

1. any nonhuman animal, especially a large, four-footed mammal
2. a contemptible person
3. Capital T capital B: the Antichrist in Revelations
4. something formidably difficult to control or deal with
**click**
5. A cross-platform program for Bayesian analysis of molecular sequences using MCMC.


---

# BEAST2

--

A *modular*, *extensible*, cross-platform package for performing Bayesian inference using MCMC with emphasis on phylogenetic analysis of molecular sequences

--

... something formidably difficult to control or deal with


???

Bayesian Evolutionary Analysis by Sampling Trees 2:  
An independent project led by the University of Auckland that is a rewrite of the original BEAST. **click** 
"A modular, extensible, cross-platform package for performing Bayesian inference using Markov Chain Monte Carlo with emphasis on phylogenetic analysis of molecular sequences"
It estimates rooted, time-measured phylogenies using strict or relaxed molecular clock models and can be used as a method of reconstructing phylogenies but is also a framework for testing evolutionary hypotheses without conditioning on a single tree topology. 

---
# BEAST2

What is it used for?  
--

* Estimate phylogenies from alignments  
--

* Estimate dates of MRCA  
--

* Estimate gene and species trees  
--

* Infer population histories  
--

* Epidemic reconstruction  
--

* Phylogeography  
--

* Simulation studies  
--

* and more!

---
layout: true
class: center, middle, inverse

# Bayesian Inference in Brief

???

Let me preface this with, I am not an expert in the use of BEAST or the statistics that underpin it. A week long course isn't nearly enough to get to that point... That being said:

---
layout: false

# Inference

*Premise:* If A is true, then B is true.  

--

*Premise:* A is true.  


???

Inference is the act of deriving logical conclusions from premises known or assumed to be true

---
layout: false

# Inference

*Premise:* If A is true, then B is true.  

*Premise:* A is true.  
***
*Inference:* B is true.

---
# Inference


*Premise:* All humans are mortal.  

--

*Premise:* Adrian is a human.  

---
# Inference


*Premise:* All humans are mortal.  

*Premise:* Adrian is a human.  
***
*Inference:* Adrian is mortal.

---
# Statistical inference


*Premise:* Squamish is a popular destination for rock climbers.  

--

*Premise:* Adrian is visiting Squamish.  

???
Statistical inference generalises this to situations where the
premises are not su?ffcient to draw conclusions without
uncertainty.

---
# Statistical inference


*Premise:* Squamish is a popular destination for rock climbers.  

*Premise:* Adrian is visiting Squamish.  
***
*Statistical Inference:* Adrian is a rock climber
--
?

???

To perform statistical inference we need a theory of plausible
reasoning.

---
# Cox's Theorem 

1. Divisibility and comparability

???
Cox suggested any theorem of plausible reasoning should satisfy the following conditions:

1. Divisibility and comparability
The plausibility of a proposition is a real number and is dependent on information we have related to the proposition.
Logical propositions can only have values true or false

---
# Cox's Theorem

1. Divisibility and comparability
2. Common sense

???

2. Common sense - All valid reasoning routes give the same result  
"Common sense" includes consistency with Aristotelian logic in the sense that logically equivalent propositions shall have the same plausibility.

---
# Cox's Theorem

1. Divisibility and comparability
2. Common sense
3. Consistency

???

3. Consistency - If the plausibility of a proposition can be derived in many ways, all the results must be equal. Equivalent states of knowledge must have equivalent degrees of plausibility

Cox's theorem has come to be used as one of the justifications for the use of Bayesian probability theory. Bayes' rule arises directly from the rules of probability. 

---
class: inverse, center, middle

# Bayes' Rule

???
Let's just skip the series of logical multiplication and summation from which we derive Bayes' Rule and discuss what it represents:

---
class: center, middle

$P(A|B) = P(B|A)P(A) / P(B)$

???

Bayes' rule or theorem provides us with a way to update our beliefs based on the arrival of new, relevant pieces of evidence. For example, if we were trying to provide the probability that a given person has cancer, we would initially just say it is whatever percent of the population has cancer. However, given additional evidence such as the fact that the person is a smoker, we can update our probability, since the probability of having cancer is higher given that the person is a smoker. This allows us to utilize prior knowledge to improve our probability estimations.

In this formula, A is the event we want the probability of, and B is the new evidence that is related to A in some way.

---
class: center, middle

$P(A|B)$

???

P(A|B) is called the posterior; this is what we are trying to estimate. In the above example, this would be the “probability of having cancer given that the person is a smoker”.

---
class: center, middle

$P(B|A)$

???

P(B|A) is called the likelihood; this is the probability of observing the new evidence, given our initial hypothesis. In the above example, this would be the “probability of being a smoker given that the person has cancer”.

---
class: center, middle

$P(A)$

???

P(A) is called the prior; this is the probability of our hypothesis without any additional prior information. In the above example, this would be the “probability of having cancer”.

---
class: center, middle

$P(B)$

???

P(B) is called the marginal likelihood; this is the total probability of observing the evidence. In the above example, this would be the “probability of being a smoker”. In many applications of Bayes Rule, this is ignored, as it mainly serves as normalization.

---
class: inverse, center, middle
background-image: url("https://media.giphy.com/media/3oGRFlpAW4sIHA02NW/giphy.gif")

???

This brings us to a point where we must more exactly define probabilities. 

Frequentists interpret probability as relative frequencies of outcomes of repeated, random (weakly controlled) "experiments"

Take for example rolling a fair, 6-sided die:  
• N: Total number of rolls.  
• n5: Total number of 5’s rolled.  
• P(d = 5) ≡ n5 /N as N → ∞

There are several problems here:
1. Experiments are assumed to be repeatable.
2. Assumes that randomness is a property of the system.
3. Completely ignores ∼ 400 years of physics.

---
# Bayesian Probability

The Bayesian interpretation treats probability as a measure of the plausibility
of propositions conditional on available information.

A single proposition can therefore have multiple probabilities depending on
the available information!

---
class: inverse, center, middle

_It is impossible for a Die, with such determin'd force and direction, not to fall on such determin'd
side, only I don't know the force and direction which makes it fall on such determin'd side, and
therefore I call it Chance, which is nothing but the want of art…_ John Arbuthnot, 1692

???

It is impossible for a Die, with such determin'd force and direction, not to fall on such determin'd
side, only I don't know the force and direction which makes it fall on such determin'd side, and
therefore I call it Chance, which is nothing but the want of art… John Arbuthnot, 1692

A measure of the plausibility of propositions conditional on available information.

---
class: inverse, center, middle

$P(A|B) = P(B|A)P(A) / P(B)$

???

In the Bayesian interpretation, probability measures a “degree of belief.” 

Bayes’ theorem then links the degree of belief in a discrete proposition (T/F) or a continuous proposition (probability density) before and after accounting for evidence. 

---
class: inverse, center, middle

$P(model | data) = P(data | model)P(model)/P(data)$

???
Putting this in terms of how BEAST2 uses bayes' theorem
---
class: inverse, center, middle

$P(data)$

???

Model evidence → P(data)
• Probability for data given model (any combination of parameters)
• Used for Bayesian model selection

---
class: inverse, center, middle

$P(model)$

???

Prior → P(model)  
• Original probability for the model parameters/components  
• Belief in our hypothesis  
• All parameters have priors, whether you specify them or not!  

For questions:
A probability!
• The probability of whatever you're interested in but in
the absence of possibly relevant data.
• In principle, any two (rational) people with access to the
same information should specify exactly the same prior.
• In practice this often isn't true.

Isn't the need for priors a problem with the Bayesian
approach?
•
NO!
• It is not possible to do inference without making
assumptions.
• Priors allow previous knowledge to be incorporated.
• Frequentist (and Likelihoodist) methods also use priors:
it's just not clear what they are!

Which prior is best?
• Only the person doing the analysis can answer this!  
• Priors encapsulate expert knowledge (or its absence).  
This is your opportunity to contribute your hard-won 
expertise to the analysis

---
class: inverse, center, middle

$P(data | model)$

???
Likelihood → P(data | model)
• Probability of data given parameters (defined by model)

---
class: inverse, center, middle

$P(model | data)$

???

Posterior → P(model | data)  
* Updated probability for the model parameters in light of the data  
* Continuous variable and therefore a probability density function

---
# Data

* Samples drawn from stochastic process  
--

* Assumed to be correct
--

* Alignments of sequencing data  
--

* Sampled at one or many time points  

???

Samples drawn from a realisation of some stochastic process
Assume that the data are correct
Typically one or more alignments of genetic sequencing data (DNA, RNA, amino acids, codons)
Sampled at one or many time points
May contain sampling location or phenotypic trait data

---
# Model

* Combines 4 separate models: 
--

  * Genealogy
--

  * Demographic 
--

  * Site
--

  * Molecular 

---
layout: true
class: center, middle
---

# Genealogy Model

---

## Sampled Rooted Time-Tree 

---
background-image: url(files/beast/fulltree.png)
background-size: contain

???

A sampled tree: reconstructed from samples drawn from study area. Have only sampled some of the available data.  
Full tree unsampled tips are X's on the full tree. Sampled tree is missing those and is smaller for it.  

---
background-image: url(files/beast/sampledtree.png)
background-size: contain

???

Sampled tree is missing those and is smaller for it.  

---
background-image: url(files/beast/timetrees.png)
background-size: contain

???

Begins at the root, lengths are measured in calendar time and each tip is fixed at a certain sampling time.  
Homochronous: All from one time point eg. outbreak, fossils, etc  
Heterochronous: Different time points

---

# Demographic Model

???

Describes population / speciation dynamics.  
How does the population demographics / species diversity
change over time?  
Uses either a backwards looking structured coalescent model or a forwards looking birth-death model  

Questions  
Coalescent:  
Given n sampling times and an estimate for estimated population over time Ne(t), out of all the ways we can connect the samples, what is the probability of the current tree?

Birth-death:
Given an estimate for the origin time, birth, death and sampling rates, if we simulate a tree forward-in-time from the origin to the time of the most recent sample, out of all the trees with n samples,
what is the probability of the current tree?
---

# Site Model

???

Combines the Site and Substitution models 
Substitution models describe rates of substitution between available characters relative to genetic distance (expected substitutions/site), as well as equilibrium frequencies of characters
Site models describe how the substitution model varies from site-to-site

---
# Clock Model

???

Determines how quickly sequences are evolving along the tree which can vary between branches  
Fixed using known sampling times  

---
class: center, middle

P(G, D, S, C | sequences) = P(sequences | G, D, S, C)P(G, D, S, C)/P(sequences)

???

So what this gives us, if we could fill out the RHS fully is the posterior distribution of probabilities for each variation of those models (genealogy, demographic, site, clock) leading to the resultant sequence alignment.    


---
class: center, middle

$P(sequences)$

???

Trouble is, we can't easily calculate the marginal likelihood or model evidence of a given set of sequences: for the complicated models that describe sequence evolution the parameter space is enormous 

---
class: inverse, center, middle

# The Solution: Markov-Chain Monte Carlo  

???

MCMC is a stochastic algorithm that performs a random walk on the posterior, preferentially sampling high-density areas

---
background-image: url(files/beast/mcmcbot.png)
background-size: contain

???
What that means:  
Choose a position on the posterior (ie. random variation of model parameters)
Randomly walk across this multi-dimensional probability landscape in a step-wise fashion
Preferentially move towards higher probability combinations of model parameters: ie. ones more likely to explain the sequence alignment given the models

---
background-image: url(files/beast/probabilitylandscape.jpg)
background-size: contain

???

Report parameter values of every Nth step.
After many many repetitions, points will be sampled in proportion to the height of the probability landscape.

---

# Many

---

# Many, Many 

---

# Many, Many, Repetitions

???
Decide on the length of the chain (total number of steps to take): Millions of steps
Decide on the sampling frequency (how often to record samples so that they are uncorrelated): Every 5-10k or so
---
background-image: url(files/beast/sticky.png)
background-size: contain

???
After run discard first 10% or so, the so called "burn in"  
Assess convergence and mixing: did it converge to a stable maximum? Was there jumping between?

---
background-image: url(files/beast/caterpillar.png)
background-size: contain

???
A successful run should look like white noise.

---
background-image: url(files/beast/figtree.png)
background-size: contain

???
Afterwards use the TreeAnnotator companion software from BEAST2 in order to take the reesults of that MCMC walk across the probabilistic landscape and convert them into a tree file which can be visualized in figtree or whatever software you prefer

---
layout: false
# Tools of the trade

* BEAST2: Software implementing MCMC for model parameter and tree inference
--

* BEAUti2: Part of BEAST2 package for setting up the input file (.xml)
--

* Tracer: Analysis of BEAST output files (.log)
--

* TreeAnnotator: Analysis of BEAST output files (.trees)
--

* FigTree, IcyTree, ggtree: Visualisation of trees (.trees)

---
class: inverse, center, middle

... something formidably difficult to control or deal with